{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VladCiocan/APIJSONParser/blob/master/aya_expanse_sft_bengali.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXDm27TkFih-"
      },
      "source": [
        "# Fine-Tuning Aya Expanse On More Languages\n",
        "\n",
        "While Aya Expanse models are highly optimized through post-training for instruction following performance on 23 languages which cover half of the world's population, these models were pre-trained on a very large corpus of text which contains many more langauges. Knowledge of many languages acquired in pre-training combined with strong, cross-lingual representations means that Aya Expanse models often perform well in languages that were not explicitly optimized for in post-training, even with little to no additional training data in that language.\n",
        "\n",
        "We can further improve the performance of Aya Expanse models on a language which is not part of the original set of 23 optimized languages by supervised fine-tuning (SFT) on a small dataset of instructions for a particular target language. In this notebook, we provide an example of fine-tuning Aya Expanse on a Bengali dataset and demonstrate that with a small amount of fine-tuning data, we can train Aya Expanse to perform well in Bengali."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tg1moVggj5sk"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install -U bitsandbytes transformers peft accelerate trl datasets sentencepiece wandb\n",
        "\n",
        "# optional for faster, lower memory usage attention\n",
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMs9uNDMHL6R"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,TrainingArguments\n",
        "from peft import LoraConfig\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izn6BYEYw4um"
      },
      "outputs": [],
      "source": [
        "USE_GPU = True\n",
        "if USE_GPU:\n",
        "    device = \"cuda:0\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "# you may want to change the following parameters depending on your GPU configuration\n",
        "\n",
        "# free T4 instance\n",
        "# QUANTIZE_4BIT = True\n",
        "# USE_GRAD_CHECKPOINTING = True\n",
        "# TRAIN_BATCH_SIZE = 2\n",
        "# TRAIN_MAX_SEQ_LENGTH = 512\n",
        "# USE_FLASH_ATTENTION = False\n",
        "# GRAD_ACC_STEPS = 16\n",
        "\n",
        "# equivalent A100 setting\n",
        "QUANTIZE_4BIT = True\n",
        "USE_GRAD_CHECKPOINTING = True\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "TRAIN_MAX_SEQ_LENGTH = 512\n",
        "USE_FLASH_ATTENTION = True\n",
        "GRAD_ACC_STEPS = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwHD5nB0FiiD"
      },
      "source": [
        "# Loading and Testing the Base Model\n",
        "Let's load the Aya Expanse 8B model and tokenizer. If you would like to use Aya Expanse 32B, change `MODEL_NAME` to `\"CohereForAI/aya-expanse-32b\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "531def06b1f7430983a2e4ba33f41f7f",
            "847b6b899bfc4e9b89b6ecb136a21385",
            "412da2e9912f4eb0ab89d44f0bb09cec",
            "1d56fddc294241f6a7cb4a300cb09afd",
            "6f83c639357f4729873f6897119532f0",
            "2551b382eca04537a3a11cd70aaf574c",
            "93e6cbabc77f4fd69ddc3dee9012cb8e",
            "da2997c847b84a32b43c377137f64b5e",
            "24f16c1efe8547f1ab36efcccda46b59",
            "cc8cb81531344463aa881093fff8c2f0",
            "f4c45b260e7a4feaaeef4c50c560641a",
            "2c5fce30aabf481098682f7fbe0fdd10"
          ]
        },
        "id": "d9a23_jiC-qG",
        "outputId": "3cf0666d-f23d-4382-b17b-c29cbe91d2f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c5fce30aabf481098682f7fbe0fdd10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You shouldn't move a model that is dispatched using accelerate hooks.\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"CohereForAI/aya-expanse-8b\"\n",
        "\n",
        "# Load Model\n",
        "quantization_config = None\n",
        "if QUANTIZE_4BIT:\n",
        "  quantization_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_quant_type=\"nf4\",\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "  )\n",
        "\n",
        "attn_implementation = None\n",
        "if USE_FLASH_ATTENTION:\n",
        "  attn_implementation=\"flash_attention_2\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "          MODEL_NAME,\n",
        "          quantization_config=quantization_config,\n",
        "          attn_implementation=attn_implementation,\n",
        "          torch_dtype=torch.bfloat16,\n",
        "        )\n",
        "model = model.to(device)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s75a8Vda-eqx"
      },
      "outputs": [],
      "source": [
        "def get_message_format(prompts):\n",
        "  messages = []\n",
        "\n",
        "  for p in prompts:\n",
        "    messages.append(\n",
        "        [{\"role\": \"user\", \"content\": p}]\n",
        "      )\n",
        "\n",
        "  return messages\n",
        "\n",
        "def generate_aya(\n",
        "      model,\n",
        "      prompts,\n",
        "      temperature=0.75,\n",
        "      top_p=1.0,\n",
        "      top_k=0,\n",
        "      max_new_tokens=1024\n",
        "    ):\n",
        "\n",
        "  messages = get_message_format(prompts)\n",
        "\n",
        "  input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "      )\n",
        "  input_ids = input_ids.to(model.device)\n",
        "  prompt_padded_len = len(input_ids[0])\n",
        "\n",
        "  gen_tokens = model.generate(\n",
        "        input_ids,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "      )\n",
        "\n",
        "  # get only generated tokens\n",
        "  gen_tokens = [\n",
        "      gt[prompt_padded_len:] for gt in gen_tokens\n",
        "    ]\n",
        "\n",
        "  gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
        "  return gen_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xszm3h9DFiiE"
      },
      "source": [
        "Let's do a quick test of the model generations in English, Vietnamese, Japanese, and Turkish, which are all part of the original set of 23 optimized languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l12EC7q-h3I",
        "outputId": "e32ee1a4-9d91-447f-9bde-c8c71c727d80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROMPT\n",
            "Write a list of three fruits and tell me about each of them\n",
            "RESPONSE\n",
            "Here is a list of three unique fruits along with some details about each:\n",
            "\n",
            "1. **Dragon Fruit (Pitaya):**\n",
            "   - Appearance: Dragon fruit is easily recognizable by its vibrant pink or yellow skin, covered in scales, resembling a dragon's skin. It has a green crown at the top. The flesh inside can be white or vibrant pink, dotted with small black seeds.\n",
            "   - Origin and Growth: Native to Central America and northern South America, dragon fruit grows on epiphytic cacti. It is now cultivated in various tropical regions worldwide. The plant produces single, large fruit that can weigh up to 2 pounds.\n",
            "   - Taste and Texture: The flesh has a mild, sweet flavor, often described as a cross between a kiwi and a pear. It has a juicy, slightly tangy taste and a creamy, buttery texture. The seeds are edible and add a crunchy element.\n",
            "\n",
            "2. **Durian:**\n",
            "   - Appearance: Durian is a spiky, oval-shaped fruit with a thick, hard shell. It is often referred to as the \"king of fruits\" due to its distinctive aroma and appearance. The shell colors can vary, but it typically includes brown, yellow, and green hues.\n",
            "   - Origin and Growth: Native to Southeast Asia, durian grows on trees in tropical forests. It is known for its strong, pungent smell, which has earned it both praise and criticism. The fruit takes about 12 months to ripen and is usually harvested when the shell becomes soft.\n",
            "   - Taste and Texture: Durian has a creamy, custard-like texture and a flavor profile that is a subject of intense debate. Some describe its taste as a blend of onions, garlic, and cream, while others find it unpleasant. Despite the controversy, durian is considered a delicacy in many Southeast Asian countries.\n",
            "\n",
            "3. **Jackfruit (Chikoo/Kacang):**\n",
            "   - Appearance: Jackfruit is the largest tree-borne fruit in the world, with some varieties weighing up to 50-80 pounds. It has a spiky, green, egg-like shape and is usually covered in a thin, edible skin. The inner flesh is divided into segments, and each segment contains a small seed.\n",
            "   - Origin and Growth: Native to South and Southeast Asia, jackfruit grows in warm, tropical climates. It is a fast-growing fruit tree and is cultivated for its fruit and seeds.\n",
            "   - Taste and Texture: Jackfruit has a unique flavor, often compared to a cross between a pineapple and a pear. The young, green fruit has a firmer texture, while ripe jackfruit becomes softer and sweeter. It's a popular ingredient in vegetarian dishes, as it has a meat-like texture when cooked.\n",
            "\n",
            "\n",
            "PROMPT\n",
            "Viết danh sách ba loại trái cây và kể cho tôi nghe về từng loại trái cây đó\n",
            "RESPONSE\n",
            "Dưới đây là ba loại trái cây cùng với mô tả về từng loại:\n",
            "\n",
            "1. **Táo (Apple):** Táo là một loại trái cây phổ biến và có nguồn gốc từ châu Á. Chúng thuộc họ Rosaceae và có nhiều giống khác nhau, với hàng ngàn biến thể về màu sắc, kích thước và vị ngọt. Táo có thể có màu đỏ, xanh, vàng hoặc kết hợp của các màu này. Chúng giàu chất xơ, vitamin C và các chất chống oxy hóa. Táo được biết đến với vị ngọt giòn và thường được tiêu thụ tươi, nhưng cũng có thể được sử dụng trong các món tráng miệng, nước ép, hoặc làm nguyên liệu cho các loại bánh và món ăn khác.\n",
            "\n",
            "2. **Cam (Orange):** Cam là một loại trái cây họ Cam chanh, có nguồn gốc từ khu vực Đông Nam Á. Chúng nổi tiếng với hương vị ngọt ngào và tinh tế, cùng với vị chua nhẹ. Cam giàu vitamin C, chất xơ và các hợp chất thực vật có lợi cho sức khỏe. Trái cam có lớp vỏ màu cam hoặc vàng, bên trong chứa nhiều phần thịt màu vàng tươi và nhiều hạt nhỏ. Cam có thể được ăn tươi, vắt lấy nước, hoặc sử dụng trong các món tráng miệng và nước ép. Chúng còn được biết đến với tác dụng tăng cường hệ miễn dịch và tốt cho sức khỏe da.\n",
            "\n",
            "3. **Dâu tây (Strawberry):** Dâu tây là loại trái cây mọng nước, thuộc họ Rosaceae, và có nguồn gốc từ châu Âu và Tây Á. Chúng có hình dạng đặc trưng với những chiếc lá tạo thành một hình chén bao quanh trái. Dâu tây có màu đỏ tươi và hương vị ngọt ngon, thường được mô tả là một trong những loại trái cây ngon nhất. Chúng giàu vitamin C, chất xơ và mangan. Dâu tây thường được ăn tươi, làm nguyên liệu cho các món tráng miệng, kem, bánh, hoặc đơn giản là rắc lên sữa chua. Ngoài ra, chúng còn được sử dụng trong sản xuất nước ép và các loại rượu vang ngọt.\n",
            "\n",
            "Mỗi loại trái cây này không chỉ mang lại hương vị tuyệt vời mà còn cung cấp nhiều lợi ích dinh dưỡng cho cơ thể con người.\n",
            "\n",
            "\n",
            "PROMPT\n",
            "3 つの果物のリストを書いて、それぞれについて教えてください\n",
            "RESPONSE\n",
            "もちろんです。以下に3つの果物のリストとそれぞれの説明を示します。\n",
            "\n",
            "1. **リンゴ**\n",
            "   - **説明**: リンゴはバラ科の果樹から取れる果物で、世界中で広く栽培されています。甘くて酸っぱい味わいが特徴で、さまざまな品種があります。栄養価も高く、ビタミンCや食物繊維が豊富です。\n",
            "\n",
            "2. **オレンジ**\n",
            "   - **説明**: オレンジはミカンの仲間の果物で、柑橘類に属します。黄色い皮とジューシーな果肉が特徴で、ビタミンCが非常に豊富に含まれています。また、抗酸化物質も多く含まれており、健康に良いとされています。\n",
            "\n",
            "3. **バナナ**\n",
            "   - **説明**: バナナはバナナ科の果物で、熱帯地域で栽培されています。柔らかくて甘い味わいが特徴で、カリウムやビタミンB6が豊富に含まれています。便秘の予防やエネルギー補給に良いとされています。\n",
            "\n",
            "これらの果物はそれぞれ異なる栄養素や特性を持っており、バランスの取れた食事に取り入れることで健康維持に役立ちます。\n",
            "\n",
            "\n",
            "PROMPT\n",
            "Üç meyveden oluşan bir liste yazın ve bana her birini anlatın\n",
            "RESPONSE\n",
            "Elbette! İşte üç farklı meyve ve kısa açıklamaları:\n",
            "\n",
            "1. **Elma**: Elma, dünyanın en yaygın olarak tüketilen meyvelerinden biridir. Çeşitli türleri ve renkleriyle bilinir; en popülerleri arasında kırmızı, yeşil ve sarı elma bulunur. Elmalar zengin bir lif kaynağıdır ve C vitamini içerir. Ayrıca, antioksidanlar ve fenolik bileşikler içerirler, bu da kalp sağlığı ve bağışıklık sistemini destekleyebilir. Elmalar tatlı ve ekşi tatlarıyla hem taze olarak yenebilir hem de çeşitli tariflerde kullanılabilir.\n",
            "\n",
            "2. **Portakal**: Portakal, portakalgiller ailesinden gelen ve C vitamini açısından zengin bir meyvedir. Kabuğunun parlak turuncu rengi ve tatlı-ekşi lezzetiyle tanınır. Portakal, günlük C vitamini ihtiyacının büyük bir kısmını karşılayabilir. Ayrıca, potasyum ve lif bakımından zengindir. Sık sık taze olarak yenir, meyve suyu olarak tüketilir veya tatlı ve tuzlu yemeklerde kullanılır. Portakal kabuğu da aromatik yağlar içeren aromatik bir bileşendir.\n",
            "\n",
            "3. **Ahududu**: Ahududu, küçük, tatlı ve ekşi bir meyvedir, genellikle koyu kırmızı veya siyah renktedir. Ahududular, antioksidanlar ve vitamin C bakımından zengindir. Bu meyveler, beyin sağlığını destekleyebilir ve iltihapla mücadelede rol oynayabilir. Ahududu, taze olarak yenmek için harika bir atıştırmalık olup, aynı zamanda muhteşem bir lezzete sahip tatlılar, reçeller ve kokteyller için de kullanılır. Ahududunun kendine özgü aroması, birçok mutfakta popüler bir seçim haline getirir.\n",
            "\n",
            "Bu meyveler, besin değerleri ve lezzetleriyle farklı tat ve sağlık avantajları sunar. Hem taze olarak tüketilebilir hem de çeşitli mutfak uygulamalarında kullanılabilirler.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test generations on langauges in Aya 23 set\n",
        "prompts = [\n",
        "    \"Write a list of three fruits and tell me about each of them\", # English\n",
        "    \"Viết danh sách ba loại trái cây và kể cho tôi nghe về từng loại trái cây đó\", # Vietnamese\n",
        "    \"3 つの果物のリストを書いて、それぞれについて教えてください\", # Japanese\n",
        "    \"Üç meyveden oluşan bir liste yazın ve bana her birini anlatın\" # Turkish\n",
        "]\n",
        "\n",
        "generations = generate_aya(model, prompts)\n",
        "\n",
        "for p, g in zip(prompts, generations):\n",
        "  print(\n",
        "      \"PROMPT\", p ,\"RESPONSE\", g, \"\\n\", sep=\"\\n\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbC7brSWFiiF"
      },
      "source": [
        "As expected, the model performs well in the languages that were part of the original set of 23 optimized languages Let's do a quick test of the model generations in Bengali, which is not part of the original set of 23 optimized languages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkEl3__Mwd8N",
        "outputId": "d4cf3e07-f148-4a57-cd69-b72acfc15b54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROMPT\n",
            "Translate from English to Bengali: \"Rates are competitive, almost always the best in the market\"\n",
            "RESPONSE\n",
            "বিক্রয় তাদের সহজ, বাধাতমকেছে মার্কেটে সবচেয়ে উন্নত\"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompts = [\n",
        "  'Translate from English to Bengali: \"Rates are competitive, almost always the best in the market\"'\n",
        "]\n",
        "\n",
        "generations = generate_aya(model, prompts)\n",
        "\n",
        "for p, g in zip(prompts, generations):\n",
        "  print(\n",
        "      \"PROMPT\", p ,\"RESPONSE\", g, \"\\n\", sep=\"\\n\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGd74gHCFiiG"
      },
      "source": [
        "While the model is able to generate a response in the correct target language of Bengali, the translation could be improved. The response translated back into English is \"Selling their simple, barrier-free solutions is the most advanced on the market\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDhgHDCNFiiG"
      },
      "source": [
        "## Dataset Setup\n",
        "\n",
        "Here we load an English to Bengali translation dataset from the Aya Collection. We filter the dataset to only include Bengali examples and define a formatting function to format the prompts to follow the chat template used by Aya Expanse models. This formatting function will be passed to the `SFTTrainer` below for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHXm3Io5zCrk"
      },
      "outputs": [],
      "source": [
        "# Load an English to Bengali translation dataset from Aya Collection\n",
        "dataset = load_dataset(\"CohereForAI/aya_collection\", \"templated_indic_sentiment\")['train']\n",
        "dataset = dataset.filter(lambda example: example['language']=='ben')\n",
        "\n",
        "def formatting_prompts_func(example):\n",
        "    output_texts = []\n",
        "    for i in range(len(example['inputs'])):\n",
        "        text = f\"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{example['inputs'][i]}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>{example['targets'][i]}\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF2pJThJFiiG"
      },
      "source": [
        "Here is an example prompt and response from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQqbSoADFiiG",
        "outputId": "70369d95-e33a-4f40-b30e-a642469d32af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROMPT\n",
            "Translate from English to Bengali: \"This boat's soundbar is still wire-connectivity for all the speakers. The HDMI port doesn't match all the devices, hence it suddenly gets disconnected sometimes.\"\n",
            "RESPONSE\n",
            "\"এই বোটের সাউন্ডবারটি এখনও সব স্পিকারের জন্য তারের সংযোগ। এইচডিএমআই পোর্ট সব ডিভাইসের সঙ্গে ম্যাচ করে না, তাই সংযোগ মাঝে মাঝে হঠাৎ বিচ্ছিন্ন হয়ে যায়।\"\n"
          ]
        }
      ],
      "source": [
        "print(f\"PROMPT\\n{dataset['inputs'][0]}\")\n",
        "print(f\"RESPONSE\\n{dataset['targets'][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-mEnFigFiiG"
      },
      "source": [
        "## SFT Model Training\n",
        "Below we configure SFT training Aya Expanse on the Bengali dataset constructed above. We use LoRA for efficient fine-tuning and thus only update and save the LoRA adapters during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9OdyDDEy7rM",
        "outputId": "49592f25-4aaf-4e21-f612-a6fe5c5865e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [720/720 13:20, Epoch 19/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.365000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.155600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.043900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.992300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.894100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.881100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.863700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.771800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.722100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.732600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.727000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.592700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.581800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.599400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.555300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.473500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.493600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.500600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.404300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.375700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.399100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.395000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.294900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.302600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.310700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.262400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.236500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.245500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.253600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.213700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.224600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.225300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.205700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.179700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.183100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.190300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.175700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.169000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.170500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.176000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.160100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.173400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.158800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.161700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.149800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.158700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.163200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.157700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.153200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.165400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.167500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.167100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.155900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.158500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.155000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.146500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.155400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.164000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.151300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.154600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.163100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.179000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.152300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.158800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.161000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.162500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.154400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.156300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.167000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.159400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.159000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.166300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/mnt/nvme/johndang_cohere_com/miniconda3/envs/gen_eval/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CohereForCausalLM(\n",
              "  (model): CohereModel(\n",
              "    (embed_tokens): Embedding(256000, 4096, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x CohereDecoderLayer(\n",
              "        (self_attn): CohereFlashAttention2(\n",
              "          (q_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (k_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (v_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (o_proj): lora.Linear4bit(\n",
              "            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "            (lora_dropout): ModuleDict(\n",
              "              (default): Identity()\n",
              "            )\n",
              "            (lora_A): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "            )\n",
              "            (lora_B): ModuleDict(\n",
              "              (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "            )\n",
              "            (lora_embedding_A): ParameterDict()\n",
              "            (lora_embedding_B): ParameterDict()\n",
              "            (lora_magnitude_vector): ModuleDict()\n",
              "          )\n",
              "          (rotary_emb): CohereRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): CohereMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): CohereLayerNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): CohereLayerNorm()\n",
              "    (rotary_emb): CohereRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=256000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training Configuration\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"results\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
        "    gradient_checkpointing=USE_GRAD_CHECKPOINTING,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=50,\n",
        "    logging_steps=10,\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    warmup_ratio=0.05,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=32,\n",
        "    r=32,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=TRAIN_MAX_SEQ_LENGTH,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    formatting_func=formatting_prompts_func\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model to disk\n",
        "trainer.model.save_pretrained(save_directory='aya-expanse-bengali-sft')\n",
        "model.config.use_cache = True\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYY2bI0PFiiH"
      },
      "source": [
        "## Testing the Fine-Tuned Model\n",
        "Now, let's load the fine-tuned model and test it on the same Bengali prompt as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174,
          "referenced_widgets": [
            "0272ba7f31a2441ab1cb5b8f77dbaacb",
            "d1bb171ddebd4f4bbeb4ed5d4b8b7076",
            "33b4fc55703746778511265e28160837",
            "7548c151f8764276ad7951e2ac80d981",
            "d972c72fef7c45998469550318661e71",
            "2811b7c68a7b4c95b91bd5690cf06577",
            "a33ccfdb735948e98a19d901d8091319",
            "c1103244cec74a299265729e630faffd",
            "340941cfc49e4ab983b73fb48c30dfe8",
            "8bb42aa84f4b4a9ab6417aed92132063",
            "b0cf428afc21468caeb664428627aaf6",
            "8e17a5ba2c9a437a8dbc99ea725327b1"
          ]
        },
        "id": "w5HGIJtRJN-y",
        "outputId": "441193fe-89fa-40ad-8585-d1f2dcf124e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e17a5ba2c9a437a8dbc99ea725327b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You shouldn't move a model that is dispatched using accelerate hooks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROMPT\n",
            "Translate from English to Bengali: \"Rates are competitive, almost always the best in the market\"\n",
            "RESPONSE\n",
            "\"দরগুলি প্রতিযোগিতামূলক, প্রায় সবসময় সেরা\"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test Bengali inference on loaded fine-tuned model\n",
        "\n",
        "# Load Model and LoRA Adapter\n",
        "quantization_config = None\n",
        "if QUANTIZE_4BIT:\n",
        "  quantization_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_quant_type=\"nf4\",\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "  )\n",
        "\n",
        "attn_implementation = None\n",
        "if USE_FLASH_ATTENTION:\n",
        "  attn_implementation=\"flash_attention_2\"\n",
        "\n",
        "loaded_sft_model = AutoModelForCausalLM.from_pretrained(\n",
        "          MODEL_NAME,\n",
        "          quantization_config=quantization_config,\n",
        "          attn_implementation=attn_implementation,\n",
        "          torch_dtype=torch.bfloat16,\n",
        "        )\n",
        "loaded_sft_model = loaded_sft_model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "loaded_sft_model.load_adapter(\"aya-expanse-bengali-sft\")\n",
        "\n",
        "\n",
        "prompts = [\n",
        "  'Translate from English to Bengali: \"Rates are competitive, almost always the best in the market\"'\n",
        "]\n",
        "\n",
        "generations = generate_aya(loaded_sft_model, prompts)\n",
        "\n",
        "for p, g in zip(prompts, generations):\n",
        "  print(\n",
        "      \"PROMPT\", p ,\"RESPONSE\", g, \"\\n\", sep=\"\\n\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYAYaw1TFiiH"
      },
      "source": [
        "The model output translated back into English is \"The rates are competitive, almost always the best\", which is much better than the original model ouput before fine-tuning as expected."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0272ba7f31a2441ab1cb5b8f77dbaacb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1bb171ddebd4f4bbeb4ed5d4b8b7076",
              "IPY_MODEL_33b4fc55703746778511265e28160837",
              "IPY_MODEL_7548c151f8764276ad7951e2ac80d981"
            ],
            "layout": "IPY_MODEL_d972c72fef7c45998469550318661e71"
          }
        },
        "1d56fddc294241f6a7cb4a300cb09afd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc8cb81531344463aa881093fff8c2f0",
            "placeholder": "​",
            "style": "IPY_MODEL_f4c45b260e7a4feaaeef4c50c560641a",
            "value": " 4/4 [00:12&lt;00:00,  2.77s/it]"
          }
        },
        "24f16c1efe8547f1ab36efcccda46b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2551b382eca04537a3a11cd70aaf574c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2811b7c68a7b4c95b91bd5690cf06577": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33b4fc55703746778511265e28160837": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1103244cec74a299265729e630faffd",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_340941cfc49e4ab983b73fb48c30dfe8",
            "value": 4
          }
        },
        "340941cfc49e4ab983b73fb48c30dfe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "412da2e9912f4eb0ab89d44f0bb09cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da2997c847b84a32b43c377137f64b5e",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24f16c1efe8547f1ab36efcccda46b59",
            "value": 4
          }
        },
        "531def06b1f7430983a2e4ba33f41f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_847b6b899bfc4e9b89b6ecb136a21385",
              "IPY_MODEL_412da2e9912f4eb0ab89d44f0bb09cec",
              "IPY_MODEL_1d56fddc294241f6a7cb4a300cb09afd"
            ],
            "layout": "IPY_MODEL_6f83c639357f4729873f6897119532f0"
          }
        },
        "6f83c639357f4729873f6897119532f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7548c151f8764276ad7951e2ac80d981": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bb42aa84f4b4a9ab6417aed92132063",
            "placeholder": "​",
            "style": "IPY_MODEL_b0cf428afc21468caeb664428627aaf6",
            "value": " 4/4 [00:11&lt;00:00,  2.57s/it]"
          }
        },
        "847b6b899bfc4e9b89b6ecb136a21385": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2551b382eca04537a3a11cd70aaf574c",
            "placeholder": "​",
            "style": "IPY_MODEL_93e6cbabc77f4fd69ddc3dee9012cb8e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8bb42aa84f4b4a9ab6417aed92132063": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93e6cbabc77f4fd69ddc3dee9012cb8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a33ccfdb735948e98a19d901d8091319": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0cf428afc21468caeb664428627aaf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1103244cec74a299265729e630faffd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc8cb81531344463aa881093fff8c2f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1bb171ddebd4f4bbeb4ed5d4b8b7076": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2811b7c68a7b4c95b91bd5690cf06577",
            "placeholder": "​",
            "style": "IPY_MODEL_a33ccfdb735948e98a19d901d8091319",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d972c72fef7c45998469550318661e71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da2997c847b84a32b43c377137f64b5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4c45b260e7a4feaaeef4c50c560641a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}